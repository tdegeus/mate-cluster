
<!DOCTYPE html>


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Monitoring and etiquette &mdash; Cluster manual  documentation</title>
    
    <link rel="stylesheet" href="_static/basic.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/bootswatch-3.1.0/united/bootstrap.min.css" type="text/css" />
    <link rel="stylesheet" href="_static/bootstrap-sphinx.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '',
        VERSION:     '',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script type="text/javascript" src="_static/js/jquery-1.11.0.min.js"></script>
    <script type="text/javascript" src="_static/js/jquery-fix.js"></script>
    <script type="text/javascript" src="_static/bootstrap-3.1.0/js/bootstrap.min.js"></script>
    <script type="text/javascript" src="_static/bootstrap-sphinx.js"></script>
    <link rel="top" title="Cluster manual  documentation" href="index.html" />
    <link rel="next" title="FAQ and tips" href="tips.html" />
    <link rel="prev" title="The queuing system" href="queuing.html" />

<link rel="stylesheet" type="text/css" href="_static/custom.css" />
<link rel="stylesheet" type="text/css" href="_static/colorbox/colorbox.css" />

<script type="text/javascript" src="_static/colorbox/jquery.colorbox-min.js"></script>

<script type="text/javascript">
	function endsWith(str, suffix) {
	    return str.indexOf(suffix, str.length - suffix.length) !== -1;
	}

    $(document).ready(function() {
    	var im, imsrc;
    	$('img').each(function() {
    		im = $(this);
    		imsrc = im.attr('src');
    		imalt = im.attr('alt');
    		if (endsWith(imsrc, 'svg')) {
    			im.wrap($('<a>').attr({'href': imsrc, 'title': imalt}).addClass('svglink'));
    		}
        if (endsWith(imsrc, 'png')) {
    			im.wrap($('<a>').attr({'href': imsrc, 'title': imalt}).addClass('pnglink'));
    		}
    	});

    	$('a.svglink').colorbox({
    		'width': '85%',
    		'height': '85%',
    		'photo': true});

      $('a.pnglink').colorbox({
    		'width': '85%',
    		'height': '85%',
    		'photo': true});


    });

</script>

  </head>
  <body>

  <div id="navbar" class="navbar navbar-inverse navbar-default navbar-fixed-top">
    <div class="container">
      <div class="navbar-header">
        <!-- .btn-navbar is used as the toggle for collapsed navbar content -->
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".nav-collapse">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="index.html"><img src="_static/MateLogo.png">
          Cluster manual</a>
        <span class="navbar-text navbar-version pull-left"><b></b></span>
      </div>

        <div class="collapse navbar-collapse nav-collapse">
          <ul class="nav navbar-nav">
            <li class="divider-vertical"></li>
            
            
              <li class="dropdown globaltoc-container">
  <a role="button"
     id="dLabelGlobalToc"
     data-toggle="dropdown"
     data-target="#"
     href="index.html">Site <b class="caret"></b></a>
  <ul class="dropdown-menu globaltoc"
      role="menu"
      aria-labelledby="dLabelGlobalToc"><ul class="current">
<li class="toctree-l1"><a class="reference internal" href="mate.html">MaTe clusters</a></li>
<li class="toctree-l1"><a class="reference internal" href="queuing.html">The queuing system</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="">Monitoring and etiquette</a></li>
<li class="toctree-l1"><a class="reference internal" href="tips.html">FAQ and tips</a></li>
<li class="toctree-l1"><a class="reference internal" href="linux_bash.html">Linux</a></li>
<li class="toctree-l1"><a class="reference internal" href="linux_bash.html#the-bash-shell">The BASH-shell</a></li>
<li class="toctree-l1"><a class="reference internal" href="languages.html">Language specific remarks/examples</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="people.html">Cluster users</a></li>
</ul>
</ul>
</li>
              
                <li class="dropdown">
  <a role="button"
     id="dLabelLocalToc"
     data-toggle="dropdown"
     data-target="#"
     href="#">Page <b class="caret"></b></a>
  <ul class="dropdown-menu localtoc"
      role="menu"
      aria-labelledby="dLabelLocalToc"><ul>
<li><a class="reference internal" href="#">Monitoring and etiquette</a><ul>
<li><a class="reference internal" href="#understand-your-own-scripts">Understand your own scripts</a></li>
<li><a class="reference internal" href="#claim-what-you-use-and-use-what-you-claim">Claim what you use and use what you claim</a></li>
<li><a class="reference internal" href="#limit-network-traffic">Limit network traffic</a></li>
<li><a class="reference internal" href="#monitor-jobs">Monitor jobs</a></li>
<li><a class="reference internal" href="#monitor-resources">Monitor resources</a></li>
<li><a class="reference internal" href="#limit-disk-usage">Limit disk-usage</a></li>
</ul>
</li>
</ul>
</ul>
</li>
              
            
            
              
                
  <li>
    <a href="queuing.html" title="Previous Chapter: The queuing system"><span class="glyphicon glyphicon-chevron-left visible-sm"></span><span class="hidden-sm">&laquo; The queuing syst...</span>
    </a>
  </li>
  <li>
    <a href="tips.html" title="Next Chapter: FAQ and tips"><span class="glyphicon glyphicon-chevron-right visible-sm"></span><span class="hidden-sm">FAQ and tips &raquo;</span>
    </a>
  </li>
              
            
            
            
            
              <li class="hidden-sm">
<div id="sourcelink">
  <a href="_sources/etiquette.txt"
     rel="nofollow">Source</a>
</div></li>
            
          </ul>

          
            
<form class="navbar-form navbar-right" action="search.html" method="get">
 <div class="form-group">
  <input type="text" name="q" class="form-control" placeholder="Search" />
 </div>
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
</form>
          
        </div>
    </div>
  </div>

<div class="container">
  <div class="row">
      <div class="col-md-3">
        <div id="sidebar" class="bs-sidenav" role="complementary"><ul>
<li><a class="reference internal" href="#">Monitoring and etiquette</a><ul>
<li><a class="reference internal" href="#understand-your-own-scripts">Understand your own scripts</a></li>
<li><a class="reference internal" href="#claim-what-you-use-and-use-what-you-claim">Claim what you use and use what you claim</a></li>
<li><a class="reference internal" href="#limit-network-traffic">Limit network traffic</a></li>
<li><a class="reference internal" href="#monitor-jobs">Monitor jobs</a></li>
<li><a class="reference internal" href="#monitor-resources">Monitor resources</a></li>
<li><a class="reference internal" href="#limit-disk-usage">Limit disk-usage</a></li>
</ul>
</li>
</ul>

<div id="sourcelink">
  <a href="_sources/etiquette.txt"
     rel="nofollow">Source</a>
</div>
<form action="search.html" method="get">
 <div class="form-group">
  <input type="text" name="q" class="form-control" placeholder="Search" />
 </div>
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
</form>
        </div>
      </div>
    <div class="col-md-9">
      
  <div class="section" id="monitoring-and-etiquette">
<span id="etiquette"></span><h1>Monitoring and etiquette<a class="headerlink" href="#monitoring-and-etiquette" title="Permalink to this headline">¶</a></h1>
<div class="contents local topic" id="outline">
<p class="topic-title first"><strong>Outline</strong></p>
<ul class="simple">
<li><a class="reference internal" href="#understand-your-own-scripts" id="id1">Understand your own scripts</a></li>
<li><a class="reference internal" href="#claim-what-you-use-and-use-what-you-claim" id="id2">Claim what you use and use what you claim</a></li>
<li><a class="reference internal" href="#limit-network-traffic" id="id3">Limit network traffic</a></li>
<li><a class="reference internal" href="#monitor-jobs" id="id4">Monitor jobs</a></li>
<li><a class="reference internal" href="#monitor-resources" id="id5">Monitor resources</a></li>
<li><a class="reference internal" href="#limit-disk-usage" id="id6">Limit disk-usage</a></li>
</ul>
</div>
<p>As the cluster is shared by many users extra responsibility is demanded from users. Currently, the scheduler is configured very openly, i.e. little restrictions are set. This gives users a lot of freedom, allowing exotic jobs, but also demands responsibility. Several fair usage considerations are found below.</p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p>If the nodes run out of resources, other jobs are harmed. When the jobs runs out of:</p>
<ul class="last simple">
<li><strong>CPUs</strong>: all jobs slow down considerably.</li>
<li><strong>Memory</strong>: the node crashes, all jobs running on it fail.</li>
<li><strong>Disk-space</strong>: the node cannot write any files, including files of the operating system. The node crashes, all jobs running on it fail.</li>
<li><strong>Network</strong>: all jobs on all nodes slow down considerably, copying operation of any job can fail.</li>
</ul>
</div>
<div class="section" id="understand-your-own-scripts">
<h2><a class="toc-backref" href="#outline">Understand your own scripts</a><a class="headerlink" href="#understand-your-own-scripts" title="Permalink to this headline">¶</a></h2>
<p>Be careful in recycling scripts (of somebody else). Make sure you understand all the details, ask for help if needed.</p>
</div>
<div class="section" id="claim-what-you-use-and-use-what-you-claim">
<h2><a class="toc-backref" href="#outline">Claim what you use and use what you claim</a><a class="headerlink" href="#claim-what-you-use-and-use-what-you-claim" title="Permalink to this headline">¶</a></h2>
<p>Specify all the resources you need, but specify them as close as possible to your job. This can be checked while the job is running, see: <a class="reference internal" href="#etiquette-monitor-jobs"><em>Monitor jobs</em></a>. Typical resources claimed:</p>
<ul>
<li><p class="first">CPUs (and compute-nodes):</p>
<div class="highlight-bash"><div class="highlight"><pre><span class="c">#PBS -l nodes=1:ppn=1:intel</span>
</pre></div>
</div>
<p>Note: the Intel-nodes are considerably faster than the AMD-nodes. The latter are mostly useful for parallelization or heavy-memory jobs.</p>
</li>
<li><p class="first">Memory:</p>
<div class="highlight-bash"><div class="highlight"><pre><span class="c">#PBS -l pmem=3gb</span>
<span class="c">#PBS -l pvmem=3gb</span>
</pre></div>
</div>
<p>The <tt class="docutils literal"><span class="pre">pmem</span></tt> option gives the scheduler information on the expected memory usage. This way moderate-memory jobs can be used to compensate for heavy-memory jobs on one node. The <tt class="docutils literal"><span class="pre">pvmem</span></tt> option makes sure that the jobs fails when the memory usage exceeds the memory claimed.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Always use these options if your job uses more than roughly &#8220;3gb&#8221; of memory.</p>
</div>
</li>
</ul>
</div>
<div class="section" id="limit-network-traffic">
<h2><a class="toc-backref" href="#outline">Limit network traffic</a><a class="headerlink" href="#limit-network-traffic" title="Permalink to this headline">¶</a></h2>
<p>Jobs that have many file operations (reading and/or writing of input/output files) should use a temporary folder on the compute-node rather than reading/writing to/from the head-node through the internal network (see <a class="reference internal" href="queuing.html#page-queuing-heavyio"><em>Heavy file-IO job</em></a>). If this is omitted the entire cluster will be impaired as the network bandwidth of the head node will be completely filled by read/write operations of these processes.</p>
<p>Software packages that always require this:</p>
<ul class="simple">
<li>Abaqus</li>
<li><a class="reference internal" href="languages.html#languages-marc"><em>MSC Marc</em></a></li>
</ul>
</div>
<div class="section" id="monitor-jobs">
<span id="etiquette-monitor-jobs"></span><h2><a class="toc-backref" href="#outline">Monitor jobs</a><a class="headerlink" href="#monitor-jobs" title="Permalink to this headline">¶</a></h2>
<p>One of the most critical responsibilities is to actively monitor all the user&#8217;s jobs on the cluster. This avoids killing (part of) the cluster, and helps to <em>&#8220;claim what you use and use what you claim&#8221;</em>. There are several way to do this, for example</p>
<ul class="simple">
<li>The <a class="reference internal" href="#etiquette-monitor-jobs-myqstat"><em>myqstat</em></a> command.</li>
<li><a class="reference internal" href="#etiquette-monitor-jobs-top"><em>System monitor</em></a>.</li>
<li>The ganglia website. For example: <a class="reference external" href="http://furnace.wfw.wtb.tue.nl/ganglia">http://furnace.wfw.wtb.tue.nl/ganglia</a></li>
</ul>
<div class="section" id="myqstat">
<span id="etiquette-monitor-jobs-myqstat"></span><h3>myqstat<a class="headerlink" href="#myqstat" title="Permalink to this headline">¶</a></h3>
<p>The <tt class="docutils literal"><span class="pre">myqstat</span></tt> command provides the most relevant information about running (or queued) jobs. For example:</p>
<div class="highlight-bash"><div class="highlight"><pre><span class="o">[</span>username@furnace ~<span class="o">]</span><span class="nv">$ </span>myqstat

id    , owner    , job name       , host, cpus  , mem  , pmem, S, <span class="nb">time</span>, score
------, ---------, ---------------, ----, ------, -----, ----, -, ----, -----
188370, tdegeus  , myjob          ,   11,  1:1:i,  10mb,  1gb, R,   2m,  1.00
</pre></div>
</div>
<p>Each row in the output corresponds to an individual job, in this example only one job is running. The columns provide information about the job:</p>
<ul class="simple">
<li><tt class="docutils literal"><span class="pre">id</span></tt>: the unique job-identifier.</li>
<li><tt class="docutils literal"><span class="pre">owner</span></tt>: the owner of the job (the username of the user that has submitted the job).</li>
<li><tt class="docutils literal"><span class="pre">job</span> <span class="pre">name</span></tt>: the name of the job:<ul>
<li>set by the <tt class="docutils literal"><span class="pre">-N</span></tt> option (in this example <tt class="docutils literal"><span class="pre">-N</span> <span class="pre">&quot;myjob&quot;</span></tt>),</li>
<li>if this option was not used, it corresponds to the name of the PBS-file.</li>
</ul>
</li>
<li><tt class="docutils literal"><span class="pre">host</span></tt>: the compute-node on which the job is running.</li>
<li><tt class="docutils literal"><span class="pre">cpus</span></tt>: the amount of CPU-resources reserved by the <tt class="docutils literal"><span class="pre">-l</span></tt> option (in this example  <tt class="docutils literal"><span class="pre">-l</span> <span class="pre">nodes=1:ppn=1:intel</span></tt>).</li>
<li><tt class="docutils literal"><span class="pre">mem</span></tt>: the amount of memory currently used by the job.</li>
<li><tt class="docutils literal"><span class="pre">pmem</span></tt>: the amount of memory requested by the <tt class="docutils literal"><span class="pre">-l</span></tt> option (in this example <tt class="docutils literal"><span class="pre">-l</span> <span class="pre">pmem=1gb</span></tt>).</li>
<li><tt class="docutils literal"><span class="pre">S</span></tt>: status of the job, can be <tt class="docutils literal"><span class="pre">R</span></tt> for running or <tt class="docutils literal"><span class="pre">Q</span></tt> for queued.</li>
<li><tt class="docutils literal"><span class="pre">time</span></tt>: the time that the job has been running (i.e. the &#8220;walltime&#8221;).</li>
<li><tt class="docutils literal"><span class="pre">score</span></tt>: the ratio between the time that the reserved processors have been in use and the time that these processes where claimed for the job.</li>
</ul>
<p>From this output to most important things to monitor are:</p>
<ul class="simple">
<li>The <strong>memory usage</strong>. The amount of memory used by all jobs on the node should never exceed the amount of memory present, otherwise the node is killed. To optimally use the memory:<ul>
<li>use the <tt class="docutils literal"><span class="pre">-l</span> <span class="pre">pmem=&quot;...&quot;</span></tt> option whenever your job uses more than <tt class="docutils literal"><span class="pre">3gb</span></tt> of memory,</li>
<li>verify that the actual memory usage does not exceed the requested amount.</li>
</ul>
</li>
<li>The <strong>score</strong>. The score is defined such that an &#8220;optimal&#8221; job receives a score of 1. If the score:<ul>
<li>&lt;&lt; 1: the CPUs spend most time waiting for a process. This can occur when more CPUs have been reserved than used by the job, or when the job has been inefficiently parallelized.</li>
<li>&gt; 1: the job is using more CPUs than reserved for the job. This slows down all other jobs on the compute-node. This may occur when the job has been inappropriately parallelized.</li>
</ul>
</li>
</ul>
<p>To obtain more information about the job available to the queuing system use:</p>
<div class="highlight-bash"><div class="highlight"><pre><span class="o">[</span>username@furnace ~<span class="o">]</span><span class="nv">$ </span>myqstat -f jobid  <span class="c"># information for a specific job-id</span>
</pre></div>
</div>
<p>For the example above</p>
<div class="highlight-bash"><div class="highlight"><pre><span class="o">[</span>username@furnace ~<span class="o">]</span><span class="nv">$ </span>myqstat -f 188370
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p>The <tt class="docutils literal"><span class="pre">myqstat</span></tt> command is available for all users, and by default in the &#8220;path&#8221; (i.e. &#8220;installed&#8221;). If the <tt class="docutils literal"><span class="pre">myqstat</span></tt> command does not work, try</p>
<div class="highlight-bash"><div class="highlight"><pre><span class="nb">export </span><span class="nv">PATH</span><span class="o">=</span><span class="s2">&quot;/usr/local/bin&quot;</span>:<span class="nv">$PATH</span>
</pre></div>
</div>
<p>(to make this default add this to the <tt class="docutils literal"><span class="pre">~/.bashrc</span></tt> file).</p>
<p><a class="reference download internal" href="_downloads/myqstat1"><tt class="xref download docutils literal"><span class="pre">source:</span> <span class="pre">myqstat</span></tt></a></p>
<p class="last"><a class="reference download internal" href="_downloads/gpbs1.py"><tt class="xref download docutils literal"><span class="pre">source:</span> <span class="pre">gpbs.py</span></tt></a></p>
</div>
</div>
<div class="section" id="system-monitor">
<span id="etiquette-monitor-jobs-top"></span><h3>System monitor<a class="headerlink" href="#system-monitor" title="Permalink to this headline">¶</a></h3>
<p>To verify that the job is running the expected processes (and only these resources) common Linux system monitoring methods can be applied. In the case that the job is doing something unexpected, for example uses much more (or less) memory or obtains a score much different than 1, this is usually the first start.</p>
<p>The first step is to login to the compute-node. For the example above:</p>
<div class="highlight-bash"><div class="highlight"><pre><span class="o">[</span>username@furnace ~<span class="o">]</span><span class="nv">$ </span>compute-0-11
</pre></div>
</div>
<p>where <tt class="docutils literal"><span class="pre">11</span></tt> should be modified to the job-host. The next step uses:</p>
<ul>
<li><p class="first">The <tt class="docutils literal"><span class="pre">top</span></tt> command is used to monitor the most important processes running on the node:</p>
<div class="highlight-bash"><div class="highlight"><pre><span class="o">[</span>username@compute-0-11 ~<span class="o">]</span><span class="nv">$ </span>top
</pre></div>
</div>
<p>use <tt class="docutils literal"><span class="pre">q</span></tt> to exit.</p>
</li>
<li><p class="first">The <tt class="docutils literal"><span class="pre">ps</span></tt> command to list all the (user&#8217;s) processes:</p>
<div class="highlight-bash"><div class="highlight"><pre><span class="o">[</span>username@compute-0-11 ~<span class="o">]</span><span class="nv">$ </span>ps
</pre></div>
</div>
<p>To list all the user&#8217;s processes on the compute-node:</p>
<div class="highlight-bash"><div class="highlight"><pre><span class="o">[</span>username@compute-0-11 ~<span class="o">]</span><span class="nv">$ </span>ps aux | grep <span class="sb">`</span>whoami<span class="sb">`</span>
</pre></div>
</div>
</li>
</ul>
</div>
</div>
<div class="section" id="monitor-resources">
<h2><a class="toc-backref" href="#outline">Monitor resources</a><a class="headerlink" href="#monitor-resources" title="Permalink to this headline">¶</a></h2>
<p>An important part of monitoring jobs is to monitor the status (or &#8220;health&#8221;) of the compute-nodes that are being used by the jobs. If for example your job is using a significant amount of memory and another job (of another user) that also uses a lot of memory is assigned to the same compute-node, the node may run out-of-memory, thus failing all jobs running on that node. When properly monitored, a job that has not been running a long time can be killed by user intervention, thus avoiding killing all other jobs. Furthermore, it is important to monitor the amount of resources claimed with respect to the availability to avoid clogging-up the cluster, thus blocking other users. There are three ways of monitoring resources:</p>
<ul>
<li><p class="first">The <a class="reference internal" href="#etiquette-monitor-resources-myqstat"><em>myqstat -N</em></a> command or <tt class="docutils literal"><span class="pre">pbsnodes</span></tt> command  (discussed below).</p>
</li>
<li><p class="first">The cluster&#8217;s website</p>
<p><a class="reference external" href="http://furnace.wfw.wtb.tue.nl/ganglia">furnace.wfw.wtb.tue.nl/ganglia</a></p>
</li>
<li><p class="first">Logging in to the compute-node, and for example:</p>
<div class="highlight-bash"><div class="highlight"><pre><span class="o">[</span>username@furnace ~<span class="o">]</span><span class="nv">$ </span>ssh compute-0-11

<span class="o">[</span>username@compute-0-11 ~<span class="o">]</span><span class="nv">$ </span>df -h      <span class="c"># free disk space</span>
<span class="o">[</span>username@compute-0-11 ~<span class="o">]</span><span class="nv">$ </span>du -hs *   <span class="c"># directory size</span>
<span class="o">[</span>username@compute-0-11 ~<span class="o">]</span><span class="nv">$ </span>top        <span class="c"># system monitor</span>
</pre></div>
</div>
</li>
</ul>
<div class="section" id="myqstat-n">
<span id="etiquette-monitor-resources-myqstat"></span><h3>myqstat -N<a class="headerlink" href="#myqstat-n" title="Permalink to this headline">¶</a></h3>
<p>The <tt class="docutils literal"><span class="pre">myqstat</span> <span class="pre">-N</span></tt> command is a wrapper of the <tt class="docutils literal"><span class="pre">pbsnodes</span></tt> command. It lists the state and the total, used, and available resources of each of the compute-nodes. A typical output is as follows:</p>
<div class="highlight-bash"><div class="highlight"><pre><span class="o">[</span>username@furnace ~<span class="o">]</span><span class="nv">$ </span>myqstat -N

Node, State        , Type , Ctot, Cused, Cfree, Score, Mtot , Mused, Mem%
----, -------------, -----, ----, -----, -----, -----, -----, -----, ----
   0, job-exclusive,   amd,   24,    24,     0,  0.06, 132gb,  19gb, 0.10
   1, job-exclusive,   amd,   24,    24,     0,  0.06, 132gb,  20gb, 0.10
...
-----------------------------------------------------
number of CPUs total    : 416 <span class="o">(</span> 240 amd / 176 intel <span class="o">)</span>
number of CPUs offline  :   8 <span class="o">(</span>   0 amd /   8 intel <span class="o">)</span>
number of CPUs online   : 408 <span class="o">(</span> 240 amd / 168 intel <span class="o">)</span>
number of CPUs working  : 299 <span class="o">(</span> 198 amd / 101 intel <span class="o">)</span>
number of CPUs free     : 109 <span class="o">(</span>  42 amd /  67 intel <span class="o">)</span>
</pre></div>
</div>
<p>The rows correspond to individual compute-nodes. The columns denote:</p>
<ul class="simple">
<li><tt class="docutils literal"><span class="pre">Node</span></tt>: the node-number of the compute-node.</li>
<li><tt class="docutils literal"><span class="pre">State</span></tt>: the status of the compute-node, for example:<ul>
<li><tt class="docutils literal"><span class="pre">free</span></tt>: one of more CPUs available for new jobs,</li>
<li><tt class="docutils literal"><span class="pre">job-exclusive</span></tt>: all CPUs are in use for jobs,</li>
<li><tt class="docutils literal"><span class="pre">down</span></tt>: down for maintenance,</li>
<li><tt class="docutils literal"><span class="pre">offline</span></tt>: down for maintenance.</li>
</ul>
</li>
<li><tt class="docutils literal"><span class="pre">Type</span></tt>: CPU-type, on <tt class="docutils literal"><span class="pre">furnace</span></tt>: <tt class="docutils literal"><span class="pre">amd</span></tt> or <tt class="docutils literal"><span class="pre">intel</span></tt>.</li>
<li><tt class="docutils literal"><span class="pre">Ctot</span></tt>: total number of CPUs in the node.</li>
<li><tt class="docutils literal"><span class="pre">Cused</span></tt>: number of CPUs in use for jobs.</li>
<li><tt class="docutils literal"><span class="pre">Cfree</span></tt>: number of CPUs available for new jobs.</li>
<li><tt class="docutils literal"><span class="pre">Score</span></tt>: the ratio of time that the reserved processors have been in use and the time that these processes where claimed by jobs. Should be around 1, otherwise there is potential mis-use, see <a class="reference internal" href="#etiquette-monitor-jobs-myqstat"><em>myqstat</em></a>.</li>
<li><tt class="docutils literal"><span class="pre">Mtot</span></tt>: total amount of memory in the node.</li>
<li><tt class="docutils literal"><span class="pre">Mused</span></tt>: amount of memory in use by jobs of the node.</li>
<li><tt class="docutils literal"><span class="pre">Mem%</span></tt>: relative amount of memory used by jobs on the node. If this value reaches 1 the node is killed.</li>
</ul>
<p>Below the list of nodes an overall summary is available, that can be used to compare the used resources to the total amount of available resources.</p>
<p>This give a lot of important information about the jobs. It <strong>does not</strong> list the amount of hard-disk space still available, or the amount of data sent over the internal network by jobs. Therefore information from the <tt class="docutils literal"><span class="pre">ganglia</span></tt> command can be included in the output as follows:</p>
<div class="highlight-bash"><div class="highlight"><pre><span class="o">[</span>username@furnace ~<span class="o">]</span><span class="nv">$ </span>myqstat -N --long

Node, State        , Type , Ctot, Cused, Cfree, Score, Mtot , Mused, Mem%, HDtot, HDused, HD% , Network
----, -------------, -----, ----, -----, -----, -----, -----, -----, ----, -----, ------, ----, -------
   0, job-exclusive,   amd,   24,    24,     0,  0.05, 132gb,  19gb, 0.10, 177gb,   81gb, 0.46,     8kb
...
</pre></div>
</div>
<p>This list the additional columns:</p>
<ul class="simple">
<li><tt class="docutils literal"><span class="pre">HDtot</span></tt>: total amount of disk-space in the node.</li>
<li><tt class="docutils literal"><span class="pre">HDused</span></tt>: total amount of disk-space used on the node.</li>
<li><tt class="docutils literal"><span class="pre">HD%</span></tt>: ratio of disk-space used on the node. If this value reaches 1, the node is killed (immediately).</li>
<li><tt class="docutils literal"><span class="pre">Network</span></tt>: total amount of data sent over the network each second.</li>
</ul>
</div>
</div>
<div class="section" id="limit-disk-usage">
<h2><a class="toc-backref" href="#outline">Limit disk-usage</a><a class="headerlink" href="#limit-disk-usage" title="Permalink to this headline">¶</a></h2>
<div class="section" id="user-data-on-the-head-node">
<h3>User-data on the head-node<a class="headerlink" href="#user-data-on-the-head-node" title="Permalink to this headline">¶</a></h3>
<p>Move your data from the cluster to your own computer when your jobs are done. Although there is a lot of harddisk space available on the cluster, the disks can and do get full, blocking other users from using the cluster. You can check the amount of free disk space by typing <tt class="docutils literal"><span class="pre">df</span> <span class="pre">-h</span></tt> (on the head node) and check the amount of free space on the line that says <tt class="docutils literal"><span class="pre">/state/partition1</span></tt>. You can check the amount of space you are using by typing <tt class="docutils literal"><span class="pre">du</span> <span class="pre">-sch</span> <span class="pre">*</span></tt> from your home directory.</p>
</div>
<div class="section" id="user-data-on-compute-nodes">
<span id="etiquette-monitor-resources-rocks"></span><h3>User-data on compute-nodes<a class="headerlink" href="#user-data-on-compute-nodes" title="Permalink to this headline">¶</a></h3>
<p>Besides the &#8220;home&#8221; folder on the head-node, user-data can be located on the hard-disks of the individual compute-nodes. This data can be the temporary data of running jobs, copied to limit network traffic and optimize the job performance (see <a class="reference internal" href="queuing.html#page-queuing-heavyio"><em>Heavy file-IO job</em></a>). However when jobs have failed <em>without</em> copying and removing the temporary data, the temporary data is left behind on the compute-node. This is highly unwanted as it is of no use, frequently forgotten, and disk-space on the compute-nodes is limited. To delete this data:</p>
<ul>
<li><p class="first">Log on to the compute node by typing</p>
<div class="highlight-bash"><div class="highlight"><pre><span class="o">[</span>username@furnance ~<span class="o">]</span><span class="nv">$ </span>ssh compute-0-n
</pre></div>
</div>
<p>where <tt class="docutils literal"><span class="pre">n</span></tt> should be replaced by the number of the compute-node. Then clean up your files in the <tt class="docutils literal"><span class="pre">/state/partition1</span></tt> directory.</p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">When a user logs in on the compute-node the current folder is always the mounted furnace home folder, i.e. <tt class="docutils literal"><span class="pre">/home/username/</span></tt> rather than the local <tt class="docutils literal"><span class="pre">/state/partition1/username</span></tt> folder.</p>
</div>
</li>
<li><p class="first">Manually logging in to each of the compute-nodes that you have used for running calculations is a time-consuming task. An easier route is to run the following command, which checks each compute-node for the existence of the user folder and its contents:</p>
<div class="highlight-bash"><div class="highlight"><pre><span class="o">[</span>username@furnace ~<span class="o">]</span><span class="nv">$ </span>rocks run host <span class="s2">&quot;ls /state/partition1/`whoami`&quot;</span>
</pre></div>
</div>
<p>To suppress warnings, use for example grep:</p>
<div class="highlight-bash"><div class="highlight"><pre><span class="o">[</span>username@furnace ~<span class="o">]</span><span class="nv">$ </span>rocks run host <span class="s2">&quot;ls /state/partition1/`whoami`&quot;</span> | grep -v <span class="s2">&quot;Warning&quot;</span> | grep -v <span class="s2">&quot;xauth&quot;</span> | grep -v <span class="s2">&quot;No such file or directory&quot;</span>
</pre></div>
</div>
<p>The output (in case temporary data is left behind on <tt class="docutils literal"><span class="pre">compute-0-16</span></tt>):</p>
<div class="highlight-bash"><div class="highlight"><pre>compute-0-23:
compute-0-19:
compute-0-16: 188370
compute-0-32:
compute-0-22:
compute-0-25:
...
</pre></div>
</div>
<p>Any files remaining on the compute-nodes (<em>also including currently running jobs</em>) will be listed behind the compute-node name. Please make sure that all files that are no longer in use are removed. For the example above the following command could be used:</p>
<div class="highlight-bash"><div class="highlight"><pre><span class="o">[</span>username@furnace ~<span class="o">]</span><span class="nv">$ </span>ssh compute-0-16 <span class="s1">&#39;rm -r /state/partition1/`whoami`/188370&#39;</span>
</pre></div>
</div>
</li>
</ul>
</div>
</div>
</div>


    </div>
      
  </div>
</div>
<footer class="footer">
  <div class="container">
    <p class="pull-right">
      <a href="#">Back to top</a>
      
    </p>
    <p>
        &copy; Copyright 2014, Tom de Geus.<br/>
      Created using <a href="http://sphinx.pocoo.org/">Sphinx</a> 1.1.3.<br/>
    </p>
  </div>
</footer>
  </body>
</html>